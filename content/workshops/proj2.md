---
title: "Introduction to GenAI and Simple LLM Inference on CPU and Fine-tuning of LLM Model to Create a Custom Chatbot"
description: "This project focuses on developing and optimizing the Dolly-v2-3b Large Language Model (LLM) for creative text generation. Trained on a specialized dataset of 15,000 instruction/response pairs, the 3 billion parameter model excels in generating engaging narratives. Integration with Intel Extension for Transformers enhances performance, optimizing hardware use for faster inference. Evaluation metrics like eval_loss and eval_ppl confirm its accuracy and context-sensitivity. Benchmarks show low latency and high throughput, processing 100 samples in 14.16 seconds at 7.061 samples per second. Ethical fine-tuning promotes unbiased storytelling, ensuring socially conscious outputs"
time: 3 Months
topics:
  - Machine Learning
  - Artificial Intelligence
  - Large Language Models
  - PeFT, QLoRA
  - Intel OneAPI
# onStartPage: true
---

### Abstract

This peoject explores the development and optimization journey of a Large Language Model 
(LLM) focused on creative text generation. The primary objective is to empower the model, 
named Dolly-v2-3b, to generate engaging and coherent narratives in response to diverse 
prompts. By fine-tuning on a specialized dataset comprising 15,000 instruction/response pairs, 
curated across various domains, the model excels in tasks such as text-generation. Its 
efficiency, with 3 billion parameters, ensures high-quality responses while maintaining 
computational lightness, crucial for practical applications prioritizing responsiveness and cost
effectiveness. 
Integration with the Intel Extension for Transformers plays a pivotal role in enhancing the 
model's performance. This collaboration optimizes hardware utilization, resulting in faster 
inference times and improved efficiency during text generation tasks. Evaluation metrics like 
eval_loss and eval_ppl underscore the model's accuracy and predictive capability, showcasing 
its ability to deliver precise and contextually appropriate responses. 
Benchmarking exercises highlight the model's robustness, with metrics indicating low latency 
and high throughput during inference. For instance, the model processes 100 samples in 
approximately 14.16 seconds, achieving an average throughput of 7.061 samples per second 
and demonstrating its suitability for real-time applications requiring rapid response 
capabilities. Furthermore, this project discusses the impact of fine-tuning methodologies, 
utilizing a systematic approach to ensure the model's outputs uphold ethical standards and 
inclusivity. By embedding prompts that encourage socially conscious storytelling, the training 
process mitigates bias and promotes the creation of engaging, unbiased narratives.

### Objectives
1. **Develop a Custom Chatbot**: Create an LLM-based chatbot capable of handling user interactions with high accuracy and relevance.
2. **Simple LLM Inference on CPU**: Implement efficient LLM inference on a CPU to ensure accessibility and cost-effectiveness.
3. **Integration of Intel Extension for Transformers**: Enhance model performance and efficiency by leveraging Intel hardware optimization.

### Problem Statement Analysis
This project focuses on implementing and fine-tuning LLMs to develop a custom chatbot, emphasizing CPU inference and integration with Intel Extension for Transformers. The goal is to optimize model performance for real-world applications requiring efficient and responsive text generation capabilities.

### Generative AI
Generative AI models are pivotal in transforming data into meaningful content across various domains, including text generation, image creation, and speech synthesis. This project leverages LLMs to advance text generation capabilities, demonstrating their versatility in creative and practical applications.

### Dataset
The project utilizes a curated dataset comprising instruction-response pairs to fine-tune the Dolly-v2-3b model. This dataset enhances the model's ability to generate contextually relevant and coherent responses, tailored to specific user prompts and interactions.

### Conclusion
By exploring the intricacies of LLM development, CPU inference optimization, and integration with Intel Extension for Transformers, this project aims to showcase the capabilities of modern AI technologies in enhancing user interactions through advanced text generation and chatbot development.

### Keywords
Large Language Models, Fine-tuning, CPU Inference, Intel Extension for Transformers, Text Generation, Custom Chatbot

## Resources

For more details and to access the model, visit our [https://github.com/KushagraIsTaken/Finetuning_Dolly-v2-3b_on_Alpaca_Dataset](#).
